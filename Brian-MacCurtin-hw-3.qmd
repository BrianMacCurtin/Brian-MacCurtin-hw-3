---
title: "Homework 3"
author: "Brian MacCurtin"
toc: true
title-block-banner: true
title-block-style: default
#format: html
format: pdf
---

[Link to the Github repository](https://github.com/psu-stat380/hw-3)

---

::: {.callout-important style="font-size: 0.8em;"}
## Due: Thu, Mar 2, 2023 @ 11:59pm

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::

For this assignment, we will be using the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset from the UCI Machine Learning Repository. The dataset consists of red and white _vinho verde_ wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests

We will be using the following libraries:

```{r, message = FALSE}
rm(list = ls())
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(car)
library(glmnet)
library(corrplot)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 50 points
Regression with categorical covariate and $t$-Test
:::

###### 1.1 (5 points)

Read the wine quality datasets from the specified URLs and store them in data frames `df1` and `df2`.

```{r}
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

url2 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"


df1 <- read.csv(url1, sep= ";")
df2 <- read.csv(url2, sep= ";")
```

---

###### 1.2 (5 points)

Perform the following tasks to prepare the data frame `df` for analysis:

1. Combine the two data frames into a single data frame `df`, adding a new column called `type` to indicate whether each row corresponds to white or red wine. 
1. Rename the columns of `df` to replace spaces with underscores
1. Remove the columns `fixed_acidity` and `free_sulfur_dioxide`
1. Convert the `type` column to a factor
1. Remove rows (if any) with missing values.


```{r}
df1$type <- "white"
df2$type <- "red"


df <- rbind(df1, df2)

df <- 
  df %>%
  rename("fixed_acidity" = "fixed.acidity",
         "volatile_acidity" = "volatile.acidity",
         "citric_acid" = "citric.acid",
         "residual_sugar" = "residual.sugar",
         "free_sulfur_dioxide" = "free.sulfur.dioxide",
         "total_sulfur_dioxide" = "total.sulfur.dioxide") %>%
  select(-c(fixed_acidity,free_sulfur_dioxide)) 
 
df$type <- as.factor(df$type)
  
df <- na.omit(df)  
head(df)

dim(df)
```


Your output to `R dim(df)` should be
```
[1] 6497   11
```



---

###### 1.3 (20 points)

Recall from STAT 200, the method to compute the $t$ statistic for the the difference in means (with the equal variance assumption)

1. Using `df` compute the mean of `quality` for red and white wine separately, and then store the difference in means as a variable called `diff_mean`. 

2. Compute the pooled sample variance and store the value as a variable called `sp_squared`. 

3. Using `sp_squared` and `diff_mean`, compute the $t$ Statistic, and store its value in a variable called `t1`.


```{r}
whitemean <- mean(df$quality[df$type == "white"])
redmean <- mean(df$quality[df$type == "red"])


diff_mean <- whitemean - redmean
diff_mean

df %>%
  group_by(type) %>%
  filter(type == "white") %>%
  summarize(whitenum = n(),
            whitevar = var(quality))

df %>%
  group_by(type) %>%
  filter(type == "red") %>%
  summarize(rednum = n(),
            redevar = var(quality))

# Formula is ((whitenum-1)*whitevar + (rednum-1)*redvar)/(whitenum+rednum-2)

sp_squared <- ((4898-1)*.7843557 + (1599-1)*.6521684)/(4898+1599-2)
sp_squared

# Formula is diff_mean/(sqrt(sp_squared*(1/whitenum + 1/rednum)))

t1 <- diff_mean/(sqrt(sp_squared*(1/4898 +1/1599)))
t1
```


---

###### 1.4 (10 points)

Equivalently, R has a function called `t.test()` which enables you to perform a two-sample $t$-Test without having to compute the pooled variance and difference in means. 

Perform a two-sample t-test to compare the quality of white and red wines using the `t.test()` function with the setting `var.equal=TRUE`. Store the t-statistic in `t2`.

```{r}
t_test <- t.test(df1$quality, df2$quality, var.equal = TRUE) 
t2 <- t_test$statistic
t2
```

---

###### 1.5 (5 points)

Fit a linear regression model to predict `quality` from `type` using the `lm()` function, and extract the $t$-statistic for the `type` coefficient from the model summary. Store this $t$-statistic in `t3`.

```{r}
fit <- lm(quality ~ type, data = df)
summary(fit)

        
t3 <- summary(fit)$coefficients[2, "t value"]
t3
```


---

###### 1.6  (5 points)

Print a vector containing the values of `t1`, `t2`, and `t3`. What can you conclude from this? Why?

```{r}
c(t1, t2, t3) 
```

The first one is slightly different then the others due to rounding because I did it by hand. Otherwise, these numbers are the same, and should be the same. From the 9.685 t-value, we can conclude that we should have a significant p-value, which means that we should reject the null hypothesis. In context of the problem, we can say that type is a useful predictor for quality. We would also be rejecting that white wine has a similar mean as red wine. We can conclude that these means are significantly different from each other.


<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 25 points
Collinearity
:::


---

###### 2.1 (5 points)

Fit a linear regression model with all predictors against the response variable `quality`. Use the `broom::tidy()` function to print a summary of the fitted model. What can we conclude from the model summary?


```{r}
full_model <- lm(quality ~ ., data = df)
broom::tidy(full_model)
```

In a model with all of the potential predictors, the variables citric_acid and total_sulfur_dioxide are not statistically significant and aren't useful in predicting quality. We can also see that the variables volatile_acidity, chlorides, total_sulfur_dioxide, and density have negative slopes. Holding all other variables constant, as one of these variables increases, the quality decreases. Also, since the coefficient for typewhite is negative, this means that the quality for white wines is lower than the quality for red wines

---

###### 2.2 (10 points)

Fit two **simple** linear regression models using `lm()`: one with only `citric_acid` as the predictor, and another with only `total_sulfur_dioxide` as the predictor. In both models, use `quality` as the response variable. How does your model summary compare to the summary from the previous question?


```{r}
model_citric <- lm(quality ~ citric_acid, data = df)
summary(model_citric)
```

```{r}
model_sulfur <- lm(quality ~ total_sulfur_dioxide, data = df)
summary(model_sulfur)
```

We can see in models by themselves, the variables citric_acid and total_sulfur_acid are both significant. By themselves, they are both useful predictors of quality. Compared to the full model, the two individual models both have higher global p-vales and lower values of r-squared. Also, the residual standard error is higher compared to the full model.


---

###### 2.3 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{r}
R <-
  df %>% 
  keep(is.numeric) %>%
  cor()
  
corrplot(R, type="upper", order="hclust")
```


---

###### 2.4 (5 points)

Compute the variance inflation factor (VIF) for each predictor in the full model using `vif()` function. What can we conclude from this?


```{r}
vif(full_model)
```

Using the threshold of 2, we can see that variables such as residual_sugar, density, alcohol and type are well over this threshold and variables like volatile_acidity and total_sulfur_dioxide are also slightly over the threshold. This would suggest higher levels of correlation between these variables and the other variables, which leads to multicollinearity

<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 40 points

Variable selection
:::


---

###### 3.1 (5 points)

Run a backward stepwise regression using a `full_model` object as the starting model. Store the final formula in an object called `backward_formula` using the built-in `formula()` function in R

```{r}
backward_model <- step(full_model, direction = "backward", scope=formula(full_model))
backward_formula <- formula(backward_model)
```

---

###### 3.2 (5 points)

Run a forward stepwise regression using a `null_model` object as the starting model. Store the final formula in an object called `forward_formula` using the built-in `formula()` function in R

```{r}
null_model <- lm(quality ~ 1, data = df)
forward_model <- step(null_model, direction = "forward", scope=formula(full_model))
forward_formula <- formula(backward_model)
```



---

###### 3.3  (10 points)

1. Create a `y` vector that contains the response variable (`quality`) from the `df` dataframe. 

2. Create a design matrix `X` for the `full_model` object using the `make_model_matrix()` function provided in the Appendix. 

3. Then, use the `cv.glmnet()` function to perform LASSO and Ridge regression with `X` and `y`.

```{r}
y <- df$quality

make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}

X <- make_model_matrix(full_model)

lasso <- cv.glmnet(X, y, alpha = 1)
lasso
ridge <- cv.glmnet(X, y, alpha = 0)
ridge
```

Create side-by-side plots of the ridge and LASSO regression results. Interpret your main findings. 

```{r}
par(mfrow=c(1, 2))
plot(ridge, main = "Ridge", cex.main = .7)
plot(lasso, main = "LASSO", cex.main = .7)
```

We can see that for the LASSO plot, as lambda increases, the number of variables decreases because some of their coefficients get shrunk to zero. For the ridge plot, as lambda increases, the variable coefficients get shrunk, but not quite to zero. The ridge plot still has 10 variables even as lambda increases. We can also see that the dashed lines fall at lower lambda values for the lasso plot compared to the ridge plot

---

###### 3.4  (5 points)

Print the coefficient values for LASSO regression at the `lambda.1se` value? What are the variables selected by LASSO? 

Store the variable names with non-zero coefficients in `lasso_vars`, and create a formula object called `lasso_formula` using the `make_formula()` function provided in the Appendix. 

```{r}
lasso_coef <- coef(lasso, s = "lambda.1se")
lasso_coef

lasso_vars <- rownames(lasso_coef)[which(abs(lasso_coef) > 0)][-1]

make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

lasso_formula <- make_formula(lasso_vars)
```

The variables selected by LASSO are the ones without the dots. The variables selected are volatile_acidity, residual_sugar, chlorides, total_sulfur_dioxide, pH, sulphates, alcohol, and type.

---

###### 3.5  (5 points)

Print the coefficient values for ridge regression at the `lambda.1se` value? What are the variables selected here? 

Store the variable names with non-zero coefficients in `ridge_vars`, and create a formula object called `ridge_formula` using the `make_formula()` function provided in the Appendix. 

```{r}
ridge_coef <- coef(ridge, s = "lambda.1se")
ridge_coef

ridge_vars <- rownames(ridge_coef)[which(abs(ridge_coef) > 0)][-1]

make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

ridge_formula <- make_formula(ridge_vars)
```

Ridge selects all of the variables since there are no dots.

---

###### 3.6  (10 points)

What is the difference between stepwise selection, LASSO and ridge based on you analyses above?

We can see that the forwards and backwards methods produced the same model, which was different from LASSO and ridge. Ridge ended up with the full model, while LASSO ended up with a slightly smaller model. Also, for the stepwise method, we can see the actual steps that went into the adding or removing of each variable. For LASSO and ridge, we just end up with the final model that each produces. The variable coefficients for LASSO and ridge tend to be smaller than what they are for the stepwise methods





<br><br><br><br>
<br><br><br><br>
---

## Question 4
::: {.callout-tip}
## 70 points

Variable selection
:::

---

###### 4.1  (5 points)

Excluding `quality` from `df` we have $10$ possible predictors as the covariates. How many different models can we create using any subset of these $10$ covariates as possible predictors? Justify your answer. 

Since we have two possible outcomes for each predictor (added to the model or not), and 10 possible predictors, there are $2^{10}$ = 1024 different models. However, that includes the possibility that none of the predictors were added to the model. If we want all possible models using at least one predictor, there are 1023 possible models

---


###### 4.2  (20 points)

Store the names of the predictor variables (all columns except `quality`) in an object called `x_vars`.

```{r}
x_vars <- colnames(df %>% select(-quality))
```

Use: 

* the `combn()` function (built-in R function) and 
* the `make_formula()` (provided in the Appendix) 

to **generate all possible linear regression formulas** using the variables in `x_vars`. This is most optimally achieved using the `map()` function from the `purrr` package.

```{r}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

formulas <- map(
  1:length(x_vars),
  \(x){
    vars <- combn(x_vars, x, simplify = FALSE)
    map(vars, make_formula) 
  }
) %>% unlist()
```

If your code is right the following command should return something along the lines of:

```{r}
sample(formulas, 4) %>% as.character()

# Output:
# [1] "quality ~ volatile_acidity + residual_sugar + density + pH + alcohol"                                                 
# [2] "quality ~ citric_acid"                                                                                                
# [3] "quality ~ volatile_acidity + citric_acid + residual_sugar + total_sulfur_dioxide + density + pH + sulphates + alcohol"
# [4] "quality ~ citric_acid + chlorides + total_sulfur_dioxide + pH + alcohol + type"  
```

---

###### 4.3  (10 points)
Use `map()` and `lm()` to fit a linear regression model to each formula in `formulas`, using `df` as the data source. Use `broom::glance()` to extract the model summary statistics, and bind them together into a single tibble of summaries using the `bind_rows()` function from `dplyr`.

```{r}
library(broom)

models <- map(formulas, ~lm(.x, data = df))
summaries <- map(models, glance) %>%
  bind_rows()

```



---


###### 4.4  (5 points)

Extract the `adj.r.squared` values from `summaries` and use them to identify the formula with the _**highest**_ adjusted R-squared value.

```{r}
adj_r_squared <- summaries$adj.r.squared
highest_r_squared <- formulas[[which.max(adj_r_squared)]]
highest_r_squared
```

Store resulting formula as a variable called `rsq_formula`.

```{r}
rsq_formula <- formula(highest_r_squared)
```

---

###### 4.5  (5 points)

Extract the `AIC` values from `summaries` and use them to identify the formula with the **_lowest_** AIC value.


```{r}
aic <- summaries$AIC
lowest_aic <- formulas[[which.min(aic)]]
lowest_aic
```

Store resulting formula as a variable called `aic_formula`.


```{r}
aic_formula <- formula(lowest_aic)
```

---

###### 4.6  (15 points)

Combine all formulas shortlisted into a single vector called `final_formulas`.

```{r}
null_formula <- formula(null_model)
full_formula <- formula(full_model)

final_formulas <- c(
  null_formula,
  full_formula,
  backward_formula,
  forward_formula,
  lasso_formula, 
  ridge_formula,
  rsq_formula,
  aic_formula
)

final_formulas
```

* Are `aic_formula` and `rsq_formula` the same? How do they differ from the formulas shortlisted in question 3?

The aic_formula and rsq_formula are not the same. The aic method did not select the same variables as any other method. It was the only method to select 9 variables, leaving out citric_acid. The rsq_formula produced the same formula as the backwards and forward methods, but was different from everything else. The aic_formula and rsq_formula differ from all of the other formulas because th other formulas do a selection process of seeing whether or not variables should be added or removed to the model, based on their own different criterias. The aic_formula and rsq_formula look through every single combination of predictors, finding only the one with the lowest AIC and highest r-squared, respectively.

* Which of these is more reliable? Why? 

The model selected by forward, backward and aic is the most reliable since it was the model that was agreed on by the most methods. It also is the model with the smallest aic which means the model fits the data really well. Also, we don't have to be worried about the AIC method adding unnecessary variables to the model since there is a penalty associated with AIC when you keep on adding variables

* If we had a dataset with $10,000$ columns, which of these methods would you consider for your analyses? Why?

I would consider using LASSO and ridge regression because these two methods offer a penalty function for the number of predictors we add to the model. This means that these methods favor smaller models. Other models do this like forwards and backwards selection, but with the large number of columns we have, it would take a lot of time for these models to reach their conclusion. Also with r-squared and aic methods, it would take awhile to create every single combination of variables to look for the lowest AIC and highest r-squared. Another benefit of LASSO and ridge is that since there a lot of variables we are looking at, these methods do a good job dealing with high correlation between variables and multicollinearity.


---

###### 4.7  (10 points)


Use `map()` and `glance()` to extract the `sigma, adj.r.squared, AIC, df`, and `p.value` statistics for each model obtained from `final_formulas`. Bind them together into a single data frame `summary_table`. Summarize your main findings.

```{r}
summary_table <- map(
  final_formulas, 
  \(x) lm(x, data = df) %>%
    broom::glance() %>%
    select(sigma, adj.r.squared, AIC, df, p.value)
) %>% bind_rows()

summary_table %>% knitr::kable()
```

We can see that all the models that had p-values, had extremely significant p-values. The p-values were so small that they were approximately zero. Also we can see that a couple of methods came up with the same model. Backward, forward, and AIC methods all came up with the same model, while the ridge method was the same as the full model. We can also see that besides the null model, all the sigma values, adjusted r-squared values, and AIC values were extremely close for all the models. The sigma values ranged from .737 to .739, the adjusted r-squared values ranged from .283 to .288 and the AIC values ranged from 14483.89 to 14520.61.



:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---


# Appendix


#### Convenience function for creating a formula object

The following function which takes as input a vector of column names `x` and outputs a `formula` object with `quality` as the response variable and the columns of `x` as the covariates. 

```{r}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

#### Convenience function for `glmnet`

The `make_model_matrix` function below takes a `formula` as input and outputs a **rescaled** model matrix `X` in a format amenable for `glmnet()`

```{r}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```




::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::